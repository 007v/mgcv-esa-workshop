Generalized Additive Models
============================
author: David L Miller
css: custom.css
transition: none


Overview
=========

- What is a GAM?
- What is smoothing?
- How do GAMs work? (*Roughly*)
- Fitting and plotting simple models

```{r setup, include=FALSE}
library(knitr)
library(viridis)
library(ggplot2)
library(reshape2)
library(animation)
library(mgcv)
opts_chunk$set(cache=TRUE, echo=FALSE)
```

What is a GAM?
===============
type:section

Generalized Additive Models
============================

- Generalized: many response distributions
- Additive: terms **add** together
- Models: well, it's a model...

To GAMs from GLMs and LMs
=============================
type:section


(Generalized) Linear Models
=============================

Models that look like:

$$
y_i = \beta_0 + x_{1i}\beta_1 + x_{2i}\beta_2 + \ldots + \epsilon_i
$$

(describe the response, $y_i$, as linear combination of the covariates, $x_{ji}$, with an offset)

We can make $y_i\sim$ any exponential family distribution (Normal, Poisson, etc).

Error term $\epsilon_i$ is normally distributed (usually).

Why bother with anything more complicated?!
=============================
type:section

Is this linear?
=============================

```{r islinear, fig.width=12, fig.height=6}
set.seed(2) ## simulate some data...
dat <- gamSim(1, n=400, dist="normal", scale=1, verbose=FALSE)
dat <- dat[,c("y", "x0", "x1", "x2", "x3")]
p <- ggplot(dat,aes(y=y,x=x1)) +
      geom_point() +
      theme_minimal()
print(p)
```

Is this linear? Maybe?
=============================

```{r eval=FALSE, echo=TRUE}
lm(y ~ x1, data=dat)
```


```{r maybe, fig.width=12, fig.height=6}
p <- ggplot(dat, aes(y=y, x=x1)) + geom_point() +
      theme_minimal()
print(p + geom_smooth(method="lm"))
```



What can we do?
=============================
type:section

Adding a quadratic term?
=============================

```{r eval=FALSE, echo=TRUE}
lm(y ~ x1 + poly(x1, 2), data=dat)
```

```{r quadratic, fig.width=12, fig.height=6}
p <- ggplot(dat, aes(y=y, x=x1)) + geom_point() +
      theme_minimal()
print(p + geom_smooth(method="lm", formula=y~x+poly(x, 2)))
```




Is this sustainable?
=============================

- Adding in quadratic (and higher terms) *can* make sense
- This feels a bit *ad hoc*
- Better if we had a **framework** to deal with these issues?

```{r ruhroh, fig.width=12, fig.height=6}
p <- ggplot(dat, aes(y=y, x=x2)) + geom_point() +
      theme_minimal()
print(p + geom_smooth(method="lm", formula=y~x+poly(x, 2)))
```


[drumroll]
=============================
type:section


What does a model look like?
=============================

$$
y_i = \beta_0 + \sum_j s_j(x_{ji}) + \epsilon_i
$$

where $\epsilon_i \sim N(0, \sigma^2)$

Let's pretend that $y_i \sim \text{Normal}$

Remember that we're modelling the mean of this distribution!

Okay, but what about these "s" things?
====================================
right:45%

```{r smoothdat, fig.width=9, fig.height=6}

spdat <- melt(dat, id.vars = c("y"))
p <- ggplot(spdat,aes(y=y,x=value)) +
      geom_point() +
      theme_minimal() +
      facet_wrap(~variable, nrow=1)
print(p)
```
***
- Think $s$=**smooth**
- Want to model the covariates flexibly
- Covariates and response not necessarily linearly related!
- Want some "wiggles"

Okay, but what about these "s" things?
====================================
right:45%

```{r wsmooths, fig.width=9, fig.height=6}
print(p + geom_smooth())
```
***
- Think $s$=**smooth**
- Want to model the covariates flexibly
- Covariates and response not necessarily linearly related!
- Want some "wiggles"

What is smoothing?
===============
type:section


Straight lines vs. interpolation
=================================

```{r wiggles}
library(mgcv)
# hacked from the example in ?gam
set.seed(2) ## simulate some data... 
dat <- gamSim(1,n=50,dist="normal",scale=0.5, verbose=FALSE)
dat$y <- dat$f2 + rnorm(length(dat$f2), sd = sqrt(0.5))
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10-mean(dat$y)
ylim <- c(-4,6)

# fit some models
b.justright <- gam(y~s(x2),data=dat)
b.sp0 <- gam(y~s(x2, sp=0, k=50),data=dat)
b.spinf <- gam(y~s(x2),data=dat, sp=1e10)

curve(f2,0,1, col="blue", ylim=ylim)
points(dat$x2, dat$y-mean(dat$y))

```
***
- Want a line that is "close" to all the data
- Don't want interpolation -- we know there is "error"
- Balance between interpolation and "fit"

Splines
========

- Functions made of other, simpler functions
- **Basis functions** $b_k$, estimate $\beta_k$ 
- $s(x) = \sum_{k=1}^K \beta_k b_k(x)$
- Makes the math(s) much easier

<img src="images/addbasis.png">

Design matrices
===============

- We often write models as $X\boldsymbol{\beta}$
- $X$ is our data
- $\boldsymbol{\beta}$ are parameters we need to estimate
- For a GAM it's the same
- $X$ has columns for each basis, evaluated at each observation

Measuring wigglyness
======================

- Visually:
  - Lots of wiggles == NOT SMOOTH
  - Straight line == VERY SMOOTH
- How do we do this mathematically?
  - Derivatives!
  - (Calculus *was* a useful class afterall!)



Wigglyness by derivatives
==========================

```{r wigglyanim, results="hide"}
library(numDeriv)
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 - mean(dat$y)

xvals <- seq(0,1,len=100)

plot_wiggly <- function(f2, xvals){

  # pre-calculate
  f2v <- f2(xvals)
  f2vg <- grad(f2,xvals)
  f2vg2 <- unlist(lapply(xvals, hessian, func=f2))
  f2vg2min <- min(f2vg2) -2
  
  # now plot
  for(i in 1:length(xvals)){
    par(mfrow=c(1,3))
    plot(xvals, f2v, type="l", main="function", ylab="f")
    points(xvals[i], f2v[i], pch=19, col="red")
    
    plot(xvals, f2vg, type="l", main="derivative", ylab="df/dx")
    points(xvals[i], f2vg[i], pch=19, col="red")
    
    plot(xvals, f2vg2, type="l", main="2nd derivative", ylab="d2f/dx2")
    points(xvals[i], f2vg2[i], pch=19, col="red")
    polygon(x=c(0,xvals[1:i], xvals[i],f2vg2min),
            y=c(f2vg2min,f2vg2[1:i],f2vg2min,f2vg2min), col = "grey")
    
    ani.pause()
  }
}

saveGIF(plot_wiggly(f2, xvals), "wiggly.gif", interval = 0.2, ani.width = 800, ani.height = 400)
```

![Animation of derivatives](wiggly.gif)

What was that grey bit?
=========================

$$
\int_\mathbb{R} \left( \frac{\partial^2 f(x)}{\partial^2 x}\right)^2 \text{d}x\\
$$

(Take some derivatives of the smooth and integrate them over $x$)

(*Turns out* we can always write this as $\boldsymbol{\beta}^\text{T}S\boldsymbol{\beta}$, so the $\boldsymbol{\beta}$ is separate from the derivatives)

(Call $S$ the **penalty matrix**)

Making wigglyness matter
=========================

- $\boldsymbol{\beta}^\text{T}S\boldsymbol{\beta}$ measures wigglyness
- "Likelihood" measures closeness to the data
- Penalise closeness to the data...
- Use a **smoothing parameter** to decide on that trade-off...
  - $\lambda \beta^\text{T}S\beta$
- Estimate the $\beta_k$ terms but penalise objective
  - "closeness to data" + penalty

Smoothing parameter
=======================


```{r wiggles-plot, fig.width=15}
# make three plots, w. estimated smooth, truth and data on each
par(mfrow=c(1,3), cex.main=3.5)

plot(b.justright, se=FALSE, ylim=ylim, main=expression(lambda*plain("= just right")))
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

plot(b.sp0, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*0))
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

plot(b.spinf, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*infinity)) 
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

```

Smoothing parameter selection
==============================

- Many methods: AIC, Mallow's $C_p$, GCV, ML, REML
- Recommendation, based on simulation and practice:
  - Use REML or ML
  - Reiss \& Ogden (2009), Wood (2011)
  
<img src="images/remlgcv.png">


Maximum wiggliness
========================

- We can set **basis complexity** or "size" ($k$)
  - Maximum wigglyness
- Smooths have **effective degrees of freedom** (EDF)
- EDF < $k$
- Set $k$ "large enough"
  - Penalty does the rest


More on this in a bit...

uhoh
======
title: none
type:section

<p align="center"><img width=150% alt="spock sobbing mathematically" src="images/mathematical_sobbing.jpg"></p>

GAM summary
===========

- Straight lines suck
- Want wiggles
- Use little functions (**basis functions**) to make big functions (**smooths**)
- Need to make sure your smooths are **wiggly enough**
- Use a **penalty** to trade off wiggliness/generality 


Fitting GAMs in practice
=========================
type:section

Translating maths into R
==========================

A simple example:

$$
y_i = \beta_0 + s(x) + s(w) + \epsilon_i
$$

where $\epsilon_i \sim N(0, \sigma^2)$

Let's pretend that $y_i \sim \text{Normal}$

- inside the link: `formula = y ~ s(x) + s(w)`
- response distribution: `family=gaussian()`
- data: `data=some_data_frame` 

Putting that together
======================

```{r echo=TRUE, eval=FALSE}
my_model <- gam(y ~ s(x) + s(w),
                family = gaussian(),
                data = some_data_frame,
                method = "REML")
```

- `method="REML"` uses REML for smoothness selection (default is `"GCV.Cp"`)

What about a practical example?
================================
type:section


Pantropical spotted dolphins
==============================

- Example taken from Miller et al (2013)
- [Paper appendix](http://distancesampling.org/R/vignettes/mexico-analysis.html) has a better analysis
- Simple example here, ignoring all kinds of important stuff!

![a pantropical spotted dolphin doing its thing](images/spotteddolphin_swfsc.jpg)


Inferential aims
=================

```{r loaddat}
load("../data/mexdolphins/mexdolphins.RData")
```

```{r gridplotfn}
library(rgdal)
library(rgeos)
library(maptools)
library(plyr)
# fill must be in the same order as the polygon data
grid_plot_obj <- function(fill, name, sp){

  # what was the data supplied?
  names(fill) <- NULL
  row.names(fill) <- NULL
  data <- data.frame(fill)
  names(data) <- name

  spdf <- SpatialPolygonsDataFrame(sp, data)
  spdf@data$id <- rownames(spdf@data)
  spdf.points <- fortify(spdf, region="id")
  spdf.df <- join(spdf.points, spdf@data, by="id")

  # seems to store the x/y even when projected as labelled as
  # "long" and "lat"
  spdf.df$x <- spdf.df$long
  spdf.df$y <- spdf.df$lat

  geom_polygon(aes_string(x="x",y="y",fill=name, group="group"), data=spdf.df)
}
```

- How many dolphins are there?
- Where are the dolphins?
- What are they interested in?

```{r spatialEDA, fig.cap="", fig.width=15}

# some nearby states, transformed
#map_dat <- map_data("state",c("mississippi","florida", "louisiana", "georgia",
#                              "alabama", "texas"))
library(mapdata)
map_dat <- map_data("worldHires",c("usa","mexico"))
lcc_proj4 <- CRS("+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs ")
map_sp <- SpatialPoints(map_dat[,c("long","lat")])

# give the sp object a projection
proj4string(map_sp) <-CRS("+proj=longlat +datum=WGS84")
# re-project
map_sp.t <- spTransform(map_sp, CRSobj=lcc_proj4)
map_dat$x <- map_sp.t$long
map_dat$y <- map_sp.t$lat

pred.polys <- spTransform(pred_latlong, CRSobj=lcc_proj4) 
p <- ggplot() +
      grid_plot_obj(preddata$depth, "Depth", pred.polys) + 
      geom_line(aes(x, y, group=Transect.Label), data=mexdolphins) +
      geom_polygon(aes(x=x, y=y, group = group), fill = "#1A9850", data=map_dat) +
      geom_point(aes(x, y, size=count),
                 data=mexdolphins[mexdolphins$count>0,],
                 colour="red", alpha=I(0.7)) +
      coord_fixed(ratio=1, ylim = range(mexdolphins$y), xlim = range(mexdolphins$x)) +
      scale_fill_viridis() +
      labs(fill="Depth",x="x",y="y",size="Count") +
      theme_minimal()
#p <- p + gg.opts
print(p)
```

A simple dolphin model
===============

```{r firstdsm, echo=TRUE}
library(mgcv)
dolphins_depth <- gam(count ~ s(depth) + offset(off.set),
                      data = mexdolphins,
                      family = quasipoisson(),
                      method = "REML")
```

- count is a function of depth
- `off.set` is the effort expended
- we have count data, try quasi-Poisson distribution

What did that do?
===================

```{r echo=TRUE}
summary(dolphins_depth)
```

Plotting
================

```{r plotsmooth}
plot(dolphins_depth)
```
***
- `plot(dolphins_depth)`
- Dashed lines indicate +/- 2 standard errors
- Rug plot
- On the link scale
- EDF on $y$ axis

Thin plate regression splines
================================

- Default basis
- One basis function per data point
- Reduce # basis functions (eigendecomposition)
- Fitting on reduced problem
- Multidimensional
- Wood (2003)


Bivariate terms
================

- Assumed an additive structure
- No interaction
- We can specify `s(x,y)` (and `s(x,y,z,...)`)
- (Assuming *isotropy* here...)


Adding a term
===============

- Add a **surface** for location ($x$ and $y$)
- Just use `+` for an extra term

```{r xydsm, echo=TRUE}
dolphins_depth_xy <- gam(count ~ s(depth) + s(x, y) + offset(off.set),
                 data = mexdolphins,
                 family=tw(), method="REML")
```


Summary
===================

```{r echo=TRUE}
summary(dolphins_depth_xy)
```

Plotting
================

```{r plotsmooth-xy, fig.width=12, echo=TRUE}
plot(dolphins_depth_xy, scale=0, pages=1)
```
- `scale=0`: each plot on different scale
- `pages=1`: plot together


Plotting 2d terms... erm...
================

```{r plotsmooth-xy-biv, fig.width=15, fig.height=7, echo=TRUE}
plot(dolphins_depth_xy, select=2, cex=2, asp=1, lwd=2)
```

- `select=` picks which smooth to plot

Let's try something different
===============================

```{r plot-scheme2, echo=TRUE, fig.width=10}
plot(dolphins_depth_xy, select=2, cex=2, asp=1, lwd=2, scheme=2)
```
- `scheme=2` much better for bivariate terms
- `vis.gam()` is much more general

More complex plots
===================

```{r visgam, fig.width=15, echo=TRUE}
par(mfrow=c(1,2))
vis.gam(dolphins_depth_xy, view=c("depth","x"), too.far=0.1, phi=30, theta=45)
vis.gam(dolphins_depth_xy, view=c("depth","x"), plot.type="contour", too.far=0.1,asp=1/1000)
```


Fitting/plotting GAMs summary
=============================

- `gam` does all the work
- very similar to `glm`
- `s` indicates a smooth term
- `plot` can give simple plots
- `vis.gam` for more advanced stuff


Prediction
===========
type:section

What is a prediction?
=====================

- Evaluate the model, at a particular covariate combination
- Answering (e.g.) the question "at a given depth, how many dolphins?"
- Steps:
  1. evaluate the $s(\ldots)$ terms
  2. move to the response scale (exponentiate? Do nothing?)
  3. (multiply any offset etc)

Example of prediction
======================

- in maths:
  - Model: $\text{count}_i = A_i \exp \left( \beta_0 + s(x_i, y_i) + s(\text{Depth}_i)\right)$
  - Drop in the values of $x, y, \text{Depth}$ (and $A$)
- in R:
  - build a `data.frame` with $x, y, \text{Depth}, A$
  - use `predict()`

```{r echo=TRUE, eval=FALSE}
preds <- predict(my_model, newdat=my_data, type="response")
```

(`se.fit=TRUE` gives a standard error for each prediction)

Back to the dolphins...
=======================
type:section

Where are the dolphins?
=======================

```{r echo=TRUE}
dolphin_preds <- predict(dolphins_depth_xy, newdata=preddata, type="response")
```

```{r fig.width=20}
p <- ggplot() +
      grid_plot_obj(dolphin_preds, "N", pred.polys) + 
      geom_line(aes(x, y, group=Transect.Label), data=mexdolphins) +
      geom_polygon(aes(x=x, y=y, group = group), fill = "#1A9850", data=map_dat) +
      geom_point(aes(x, y, size=count),
                 data=mexdolphins[mexdolphins$count>0,],
                 colour="red", alpha=I(0.7)) +
      coord_fixed(ratio=1, ylim = range(mexdolphins$y), xlim = range(mexdolphins$x)) +
      scale_fill_viridis() +
      labs(fill="Predicted\ndensity", x="x", y="y", size="Count") +
      theme_minimal()
print(p)
```

(`ggplot2` code included in the slide source)

Prediction summary
==================

- Evaluate the fitted model at a given point
- Can evaluate many at once (`data.frame`)
- Don't forget the `type=...` argument!
- Obtain per-prediction standard error with `se.fit`

What about uncertainty?
========================
type:section

Without uncertainty, we're not doing statistics 
========================
type:section

Where does uncertainty come from?
=================================

- $\boldsymbol{\beta}$: uncertainty in the spline parameters
- $\boldsymbol{\lambda}$: uncertainty in the smoothing parameter

- Traditionally we've only addressed the former
- New tools let us address the latter...


Parameter uncertainty
=======================

From theory:

$$
\boldsymbol{\beta} \sim N(\hat{\boldsymbol{\beta}},  \mathbf{V}_\boldsymbol{\beta})
$$

(*caveat: the normality is only* **approximate** *for non-normal response*)


**What does this mean?** Variance for each parameter.

In `mgcv`: `vcov(model)` returns $\mathbf{V}_\boldsymbol{\beta}$.


What can we do this this?
===========================

- confidence intervals in `plot`
- standard errors using `se.fit`
- derived quantities? (see bibliography)

blah
====
title:none
type:section

<img src="images/tina-modelling.png">



The lpmatrix, magic, etc
==============================

For regular predictions:

$$
\hat{\boldsymbol{\eta}}_p = L_p \hat{\boldsymbol{\beta}}
$$

form $L_p$ using the prediction data, evaluating basis functions as we go.

(Need to apply the link function to $\hat{\boldsymbol{\eta}}_p$)

But the $L_p$ fun doesn't stop there...

[[mathematics intensifies]]
============================
type:section

Variance and lpmatrix
======================

To get variance on the scale of the linear predictor:

$$
V_{\hat{\boldsymbol{\eta}}} = L_p^\text{T} V_\hat{\boldsymbol{\beta}} L_p
$$

pre-/post-multiplication shifts the variance matrix from parameter space to linear predictor-space.

(Can then pre-/post-multiply by derivatives of the link to put variance on response scale)

Simulating parameters
======================

- $\boldsymbol{\beta}$ has a distribution, we can simulate

```{r paramsim, results="hide"}
library(mvtnorm)

# get the Lp matrix
Lp <- predict(dolphins_depth_xy, newdata=preddata, type="lpmatrix")

# how many realisations do we want?
frames <- 100

# generate the betas from the GAM "posterior"
betas <- rmvnorm(frames, coef(dolphins_depth_xy), vcov(dolphins_depth_xy))


# use a function to get animation to play nice...
anim_map <- function(){
  # loop to make plots
  for(frame in 1:frames){

    # make the prediction
    preddata$preds <- preddata$area * exp(Lp%*%betas[frame,])

    # plot it (using viridis)
    p <- ggplot() +
          grid_plot_obj(preddata$preds, "N", pred.polys) + 
          geom_polygon(aes(x=x, y=y, group = group), fill = "#1A9850", data=map_dat) +
          coord_fixed(ratio=1, ylim = range(mexdolphins$y),
                      xlim = range(mexdolphins$x)) +
          scale_fill_viridis(limits=c(0,200)) +
          labs(fill="Predicted\ndensity",x="x",y="y",size="Count") +
          theme_minimal()
    
    print(p)
  }
}

# make the animation!
saveGIF(anim_map(), "uncertainty.gif", outdir = "new", interval = 0.15, ani.width = 800, ani.height = 400)
```

![Animation of uncertainty](uncertainty.gif)

Uncertainty in smoothing parameter
==================================

- Recent work by Simon Wood
- "smoothing parameter uncertainty corrected" version of $V_\hat{\boldsymbol{\beta}}$
- In a fitted model, we have:
  - `$Vp` what we got with `vcov`
  - `$Vc` the corrected version
- Still experimental

Variance summary
================

- Everything comes from variance of parameters
- Need to re-project/scale them to get the quantities we need
- `mgcv` does most of the hard work for us
- Fancy stuff possible with a little maths
- Can include uncertainty in the smoothing parameter too

Okay, that was a lot of information
===================================
type:section

Summary
=======

- GAMs are GLMs plus some extra wiggles
- Need to make sure things are *just wiggly enough*
  - Basis + penalty is the way to do this
- Fitting looks like `glm` with extra `s()` terms
- Most stuff comes down to matrix algebra, that `mgcv` sheilds you from
- To do fancy stuff, get inside the matrices
